{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eric\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import package\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora,models\n",
    "import gensim\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from stop_words import get_stop_words\n",
    "import pandas as pd\n",
    "import json\n",
    "import warnings\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_consumerreports_1=pd.read_csv(r\"C:\\Users\\eric\\Desktop\\consumerreports_text.csv\",header=0)\n",
    "documents_consumerreports_1=documents_consumerreports_1.rename(columns={'0':'text'})\n",
    "documents_consumerreports_1 = documents_consumerreports_1.drop('1', 1)\n",
    "documents_consumerreports_1=documents_consumerreports_1[:180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#documents_consumerreports_1.head()\n",
    "len(documents_consumerreports_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_autofutures=pd.read_csv(r\"C:\\Users\\eric\\Desktop\\autofutures.csv\",header=0)\n",
    "documents_autofutures.head()\n",
    "documents_autofutures=documents_autofutures.rename(columns={'0':'text'})\n",
    "documents_autofutures=documents_autofutures[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents_autofutures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_2025AD=pd.read_csv(r\"C:\\Users\\eric\\Desktop\\2028_new_text.csv\",header=0)\n",
    "documents_2025AD=documents_2025AD.rename(columns={'0':'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_caranddriver=pd.read_csv(r\"C:\\Users\\eric\\Desktop\\caranddriver_text.csv\",header=0)\n",
    "documents_caranddriver=documents_caranddriver.rename(columns={'0':'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_raw=pd.concat([documents_consumerreports_1,documents_caranddriver,documents_autofutures,documents_2025AD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_raw=documents_raw.reset_index(drop= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1304"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어사전만들기\n",
    "en_stop = get_stop_words('en')\n",
    "en_stop.extend([\"self\", \"automaker\",\"automotive\",\"driving\", \"car\", \"autonomous\", \"vehicle\",\"driverless\",\"driver\",\"drive\",\"drivers\",\"auto\",\"vehicles\",\"cars\",\"automated\",'automatic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모든 대문자를 소문자로\n",
    "documents_raw = documents_raw.fillna('').astype(str).apply(lambda x: x.str.lower()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:2: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:2: DeprecationWarning: invalid escape sequence \\s\n",
      "<ipython-input-19-e527240fd92d>:2: DeprecationWarning: invalid escape sequence \\s\n",
      "  documents_raw['text']= documents_raw['text'].map(lambda x: re.sub('\\s+', ' ', x))\n"
     ]
    }
   ],
   "source": [
    "#불필요한거제거\n",
    "documents_raw['text']= documents_raw['text'].map(lambda x: re.sub('\\s+', ' ', x))\n",
    "documents_raw['text']= documents_raw['text'].map(lambda x: re.sub(\"\\'\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize\n",
    "Lem = WordNetLemmatizer()\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(Lem.lemmatize(token,pos=\"n\"))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents_raw['text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 제거\n",
    "import numpy as np\n",
    "def remove_stopwords(, en_stop):\n",
    "    \n",
    "    '''\n",
    "    A function for removing unused word lists from text.\n",
    "    \n",
    "    ---parameter---\n",
    "    series: Pandas Series\n",
    "    using_word_list: List for using word\n",
    "    \n",
    "    '''\n",
    "    result= []\n",
    "    for doc in series:\n",
    "        text= [word for word in doc if word not in en_stop]\n",
    "        result.append(text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopped_processed_docs =remove_stopwords(processed_docs,en_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #제거되었는지확인\n",
    "# stopped_processed_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eric\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['safety', 'technology', 'advancing', 'breakneck', 'speed', 'today', 'emergency_braking', 'system', 'sensor', 'help', 'avoid', 'collision', 'reduce', 'severity', 'fail', 'early', 'sign', 'advance', 'communication', 'allow', 'talk', 'avoid', 'tech', 'company', 'holy_grail', 'highway', 'safety', 'reduce', 'eliminate', 'crash', 'safety', 'advance', 'decade', 'people', 'died', 'roadway', 'according', 'national_highway_traffic', 'safety_administration', 'slightly', 'death', 'annually', 'historically', 'significant', 'point', 'fatality', 'national', 'safety', 'council', 'tally', 'death', 'private', 'road', 'say', 'annual', 'figure', 'actually', 'regulator', 'worked', 'safer', 'year', 'aren', 'highway', 'fatality', 'going', 'people', 'mile', 'past', 'explain', 'increase', 'entirely', 'transportation', 'expert', 'industry', 'government', 'road', 'designer', 'safety', 'regulator', 'bring', 'number']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(stopped_processed_docs, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[stopped_processed_docs], threshold=100)\n",
    " \n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    " \n",
    "# See trigram example0\n",
    "print(trigram_mod[bigram_mod[stopped_processed_docs[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(stopped_processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_words_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #불용어사전만들기\n",
    "    en_stop = get_stop_words('en')\n",
    "    en_stop.extend([\"self\", \"automaker\",\"automotive\",\"driving\", \"car\", \"autonomous\", \"vehicle\",\"driverless\",\"driver\",\"drive\",\"auto\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\eric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def nltk2wn_tag(nltk_tag):\n",
    "  if nltk_tag.startswith('J'):\n",
    "    return wordnet.ADJ\n",
    "  elif nltk_tag.startswith('V'):\n",
    "    return wordnet.VERB\n",
    "  elif nltk_tag.startswith('N'):\n",
    "    return wordnet.NOUN\n",
    "  elif nltk_tag.startswith('R'):\n",
    "    return wordnet.ADV\n",
    "  else:          \n",
    "    return None\n",
    "def lemmatize_sentence(sentence):\n",
    "  nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "  wn_tagged = map(lambda x: (x[0], nltk2wn_tag(x[1])), nltk_tagged)\n",
    "  res_words = []\n",
    "  for word, tag in wn_tagged:\n",
    "    if tag is None:            \n",
    "      res_words.append(word)\n",
    "    else:\n",
    "      res_words.append(lemmatizer.lemmatize(word, tag))\n",
    "  return \" \".join(res_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe to sent_list\n",
    "sent_text_list=[]\n",
    "result=[]\n",
    "for i in range(len(documents_raw)):\n",
    "    sent_text=sent_tokenize(documents_raw.text[i])\n",
    "    singles = [lemmatize_sentence(sent) for sent in sent_text]\n",
    "    result=[word_tokenize(sentence) for sentence in singles]\n",
    "    sent_text_list.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['automotive',\n",
       " 'safety',\n",
       " 'technology',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'be',\n",
       " 'advance',\n",
       " 'at',\n",
       " 'breakneck',\n",
       " 'speed',\n",
       " '.']"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_text_list[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1304/1304 [14:10<00:00,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "result_a= []\n",
    "input_data=[]\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(sent_text_list))):\n",
    "    for sent in sent_text_list[i]:\n",
    "        text= [word for word in sent if word not in en_stop]\n",
    "        text=[x.lower() for x in text]\n",
    "        text=[re.sub(\"[^a-zA-Z]\", \" \", j) for j in text]\n",
    "        result_a.append(text)\n",
    "    input_data.append(result_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(input_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "model = Word2Vec(input_data[0], size=100, window=5, min_count=3, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eric\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('automate', 0.9050835371017456),\n",
       " ('autopilot', 0.8854923248291016),\n",
       " ('pilot', 0.8735599517822266),\n",
       " ('cadillac', 0.8595130443572998),\n",
       " ('egg shaped', 0.8572568297386169),\n",
       " ('beta', 0.8536468148231506),\n",
       " ('self driving', 0.8521950840950012),\n",
       " ('ready', 0.8511067628860474),\n",
       " ('today', 0.8504892587661743),\n",
       " ('prototype', 0.8465949296951294)]"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('fully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
